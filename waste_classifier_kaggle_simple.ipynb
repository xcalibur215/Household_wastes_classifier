{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc7fe4da",
   "metadata": {},
   "source": [
    "# Hierarchical Waste Classifier - Vision Transformer\n",
    "\n",
    "This notebook trains a **Hierarchical Vision Transformer** for waste classification:\n",
    "- **30 fine-grained classes** (specific waste types)  \n",
    "- **7 super categories** (Metal, Paper, Glass, Plastic, Styrofoam, Organic, Textiles)\n",
    "\n",
    "## Quick Start\n",
    "**Just run all cells in sequence** - the notebook is designed to work smoothly from top to bottom.\n",
    "\n",
    "### Key Features:\n",
    "- **Dual-head architecture** with shared ViT backbone\n",
    "- **Hierarchical loss function** combining fine-grained and super-class predictions\n",
    "- **Live training progress** with tqdm bars\n",
    "- **Automatic model saving** and performance tracking\n",
    "- **Comprehensive analysis** with confusion matrices\n",
    "\n",
    "### Requirements:\n",
    "Update the `dataset_path` in the next cell to point to your dataset location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ccdb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision.transforms import v2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from timm import create_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset setup\n",
    "dataset_path = '/kaggle/input/household-waste-30-classes/images/images'\n",
    "\n",
    "# Check if dataset path exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"Dataset path not found: {dataset_path}\")\n",
    "    print(\"Please update the dataset_path variable with the correct path to your dataset\")\n",
    "else:\n",
    "    print(f\"Dataset path found: {dataset_path}\")\n",
    "\n",
    "full_dataset = ImageFolder(dataset_path)\n",
    "class_map_dict = full_dataset.class_to_idx\n",
    "class_names = list(class_map_dict.keys())\n",
    "print(f'Number of classes: {len(class_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6791446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hierarchical classification system\n",
    "SUPER_CLASSES = {\n",
    "    0: ['aerosol_cans', 'aluminum_food_cans', 'aluminum_soda_cans', 'steel_food_cans'],\n",
    "    1: ['cardboard_boxes', 'cardboard_packaging', 'magazines', 'newspaper', 'office_paper', 'paper_cups'],\n",
    "    2: ['glass_beverage_bottles', 'glass_cosmetic_containers', 'glass_food_jars'],\n",
    "    3: ['disposable_plastic_cutlery', 'plastic_cup_lids', 'plastic_detergent_bottles', 'plastic_food_containers', 'plastic_shopping_bags', 'plastic_soda_bottles', 'plastic_straws', 'plastic_trash_bags', 'plastic_water_bottles'],\n",
    "    4: ['styrofoam_cups', 'styrofoam_food_containers'],\n",
    "    5: ['coffee_grounds', 'eggshells', 'food_waste', 'tea_bags'],\n",
    "    6: ['clothing', 'shoes']\n",
    "}\n",
    "\n",
    "SUPER_CLASS_NAMES = {\n",
    "    0: 'Metal_Aluminum', 1: 'Cardboard_Paper', 2: 'Glass_Containers',\n",
    "    3: 'Plastic_Items', 4: 'Styrofoam_Products', 5: 'Organic_Waste', 6: 'Textiles_Clothing'\n",
    "}\n",
    "\n",
    "# Build mappings\n",
    "CLASS_TO_SUPER = {}\n",
    "for super_id, class_list in SUPER_CLASSES.items():\n",
    "    for class_name in class_list:\n",
    "        CLASS_TO_SUPER[class_name] = super_id\n",
    "\n",
    "CLASS_IDX_TO_SUPER_IDX = {}\n",
    "for class_name, class_idx in class_map_dict.items():\n",
    "    if class_name in CLASS_TO_SUPER:\n",
    "        super_idx = CLASS_TO_SUPER[class_name]\n",
    "        CLASS_IDX_TO_SUPER_IDX[class_idx] = super_idx\n",
    "\n",
    "print(f\"Hierarchical Classification Setup Complete:\")\n",
    "print(f\"• Fine-grained classes: {len(class_names)}\")\n",
    "print(f\"• Super classes: {len(SUPER_CLASS_NAMES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670e1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms and loaders\n",
    "train_transform = v2.Compose([\n",
    "    v2.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = v2.Compose([\n",
    "    v2.Resize((224, 224)), v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class SimpleTransformWrapper(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        image, target = self.dataset[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Split dataset\n",
    "torch.manual_seed(42)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Apply transforms\n",
    "train_dataset_transformed = SimpleTransformWrapper(train_dataset, transform=train_transform)\n",
    "test_dataset_transformed = SimpleTransformWrapper(test_dataset, transform=test_transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset_transformed, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset_transformed, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Dataset split: {len(train_dataset_transformed)} train, {len(test_dataset_transformed)} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497dfb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Vision Transformer Model\n",
    "class HierarchicalViT(nn.Module):\n",
    "    def __init__(self, num_fine_classes=30, num_super_classes=7, model_name='vit_small_patch16_224', dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.backbone = create_model(model_name, pretrained=True, num_classes=0)\n",
    "        feature_dim = self.backbone.num_features\n",
    "        \n",
    "        self.feature_processor = nn.Sequential(nn.LayerNorm(feature_dim), nn.Dropout(dropout))\n",
    "        \n",
    "        self.fine_classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim // 2), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(feature_dim // 2, num_fine_classes)\n",
    "        )\n",
    "        \n",
    "        self.super_classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim // 4), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(feature_dim // 4, num_super_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        processed_features = self.feature_processor(features)\n",
    "        fine_logits = self.fine_classifier(processed_features)\n",
    "        super_logits = self.super_classifier(processed_features)\n",
    "        return fine_logits, super_logits\n",
    "\n",
    "# Setup device and model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "waste_classifier = HierarchicalViT(num_fine_classes=30, num_super_classes=7, dropout=0.3).to(device)\n",
    "print(f\"Model created on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class HierarchicalLoss(nn.Module):\n",
    "    def __init__(self, fine_weight=0.7, super_weight=0.3, focal_gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.fine_weight = fine_weight\n",
    "        self.super_weight = super_weight\n",
    "        self.fine_loss = FocalLoss(gamma=focal_gamma)\n",
    "        self.super_loss = FocalLoss(gamma=focal_gamma)\n",
    "    \n",
    "    def forward(self, fine_logits, super_logits, fine_targets, super_targets):\n",
    "        loss_fine = self.fine_loss(fine_logits, fine_targets.long())\n",
    "        loss_super = self.super_loss(super_logits, super_targets.long())\n",
    "        total_loss = self.fine_weight * loss_fine + self.super_weight * loss_super\n",
    "        return total_loss, loss_fine, loss_super\n",
    "\n",
    "criterion_hierarchical = HierarchicalLoss(fine_weight=0.7, super_weight=0.3, focal_gamma=2.0)\n",
    "print(\"Loss functions initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78d2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train_epoch_hierarchical(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, fine_correct, super_correct, total_samples = 0, 0, 0, 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for images, fine_labels in pbar:\n",
    "        images, fine_labels = images.to(device), fine_labels.to(device)\n",
    "        super_labels = torch.tensor([CLASS_IDX_TO_SUPER_IDX.get(int(label), 0) for label in fine_labels.cpu()]).to(device)\n",
    "        \n",
    "        fine_logits, super_logits = model(images)\n",
    "        loss, _, _ = criterion(fine_logits, super_logits, fine_labels, super_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, fine_pred = torch.max(fine_logits, 1)\n",
    "        _, super_pred = torch.max(super_logits, 1)\n",
    "        fine_correct += (fine_pred == fine_labels).sum().item()\n",
    "        super_correct += (super_pred == super_labels).sum().item()\n",
    "        total_samples += images.size(0)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pbar.set_postfix({'Loss': f'{loss.item():.4f}', 'Fine': f'{100*fine_correct/total_samples:.1f}%'})\n",
    "    \n",
    "    return total_loss/len(train_loader), 100*fine_correct/total_samples, 100*super_correct/total_samples\n",
    "\n",
    "def eval_epoch_hierarchical(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, fine_correct, super_correct, total_samples = 0, 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, fine_labels in test_loader:\n",
    "            images, fine_labels = images.to(device), fine_labels.to(device)\n",
    "            super_labels = torch.tensor([CLASS_IDX_TO_SUPER_IDX.get(int(label), 0) for label in fine_labels.cpu()]).to(device)\n",
    "            \n",
    "            fine_logits, super_logits = model(images)\n",
    "            loss, _, _ = criterion(fine_logits, super_logits, fine_labels, super_labels)\n",
    "            \n",
    "            _, fine_pred = torch.max(fine_logits, 1)\n",
    "            _, super_pred = torch.max(super_logits, 1)\n",
    "            fine_correct += (fine_pred == fine_labels).sum().item()\n",
    "            super_correct += (super_pred == super_labels).sum().item()\n",
    "            total_samples += images.size(0)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss/len(test_loader), 100*fine_correct/total_samples, 100*super_correct/total_samples\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f158488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "EPOCHS = 15\n",
    "optimizer = torch.optim.AdamW(waste_classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-5)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_fine_accs, val_fine_accs = [], []\n",
    "train_super_accs, val_super_accs = [], []\n",
    "best_val_acc = 0\n",
    "\n",
    "print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_fine_acc, train_super_acc = train_epoch_hierarchical(\n",
    "        waste_classifier, train_loader, criterion_hierarchical, optimizer, device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_fine_acc, val_super_acc = eval_epoch_hierarchical(\n",
    "        waste_classifier, test_loader, criterion_hierarchical, device\n",
    "    )\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_fine_accs.append(train_fine_acc)\n",
    "    val_fine_accs.append(val_fine_acc)\n",
    "    train_super_accs.append(train_super_acc)\n",
    "    val_super_accs.append(val_super_acc)\n",
    "    \n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch:2d} | LR: {lr:.6f} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"         | Fine: {train_fine_acc:.1f}%/{val_fine_acc:.1f}% | Super: {train_super_acc:.1f}%/{val_super_acc:.1f}%\")\n",
    "    \n",
    "    current_acc = val_fine_acc + val_super_acc\n",
    "    if current_acc > best_val_acc:\n",
    "        best_val_acc = current_acc\n",
    "        torch.save(waste_classifier.state_dict(), 'best_hierarchical_model.pth')\n",
    "        print(f\"         *** New best model saved! Combined accuracy: {current_acc/2:.1f}%\")\n",
    "\n",
    "print(f\"\\nTraining completed! Best combined accuracy: {best_val_acc/2:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3156e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "if len(train_losses) > 0:\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Hierarchical Training Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    ax1.plot(epochs, train_losses, 'b-', label='Training', linewidth=2)\n",
    "    ax1.plot(epochs, val_losses, 'r-', label='Validation', linewidth=2)\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(epochs, train_fine_accs, 'g-', label='Training', linewidth=2)\n",
    "    ax2.plot(epochs, val_fine_accs, 'orange', label='Validation', linewidth=2)\n",
    "    ax2.set_title('Fine-grained Accuracy (30 classes)')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax3.plot(epochs, train_super_accs, 'purple', label='Training', linewidth=2)\n",
    "    ax3.plot(epochs, val_super_accs, 'brown', label='Validation', linewidth=2)\n",
    "    ax3.set_title('Super-class Accuracy (7 categories)')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    categories = ['Fine-grained', 'Super-class']\n",
    "    final_train = [train_fine_accs[-1], train_super_accs[-1]]\n",
    "    final_val = [val_fine_accs[-1], val_super_accs[-1]]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax4.bar(x - width/2, final_train, width, label='Training', alpha=0.8)\n",
    "    ax4.bar(x + width/2, final_val, width, label='Validation', alpha=0.8)\n",
    "    ax4.set_title('Final Accuracy Comparison')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(categories)\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Best Fine-grained: {max(val_fine_accs):.1f}%\")\n",
    "    print(f\"Best Super-class: {max(val_super_accs):.1f}%\")\n",
    "else:\n",
    "    print(\"No training data to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "gc.collect()\n",
    "print(\"Training complete! Model saved as 'best_hierarchical_model.pth'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
